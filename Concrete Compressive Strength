##  Multivariate regression model to determine car values based on a variety of characteristics
#### Data are extracted from the Cars93 df in the MASS library
library(MASS)
data(Cars93)
summary(Cars93)
#### Exploratory data analysis. Since we have only 93 observation we consider the whole data set as training dataset. Test data set will be a subset of the training dataset.
#### Let's check for column correlation
cor(Cars93$Min.Price,Cars93$Price)
cor(Cars93$Max.Price,Cars93$Price)
#### We remove Max.Price and Min.Price column from the datamodel because strongly related to Price column
Cars93<-Cars93[,setdiff(colnames(Cars93),c("Min.Price","Max.Price"))]
NAs<-sum(as.vector(apply(apply(Cars93,2,function(x) {ifelse(is.na(x),1,0)}),2,sum)))
NAs/dim(Cars93)[[1]]
#### About 13% of data contains NA. Not too much.
Let's check which column contains NA, in case they are included into the final model.
data.frame(lapply(lapply(Cars93,function(x) {ifelse(is.na(x),1,0)}),sum))
#### We notice that column Luggage.room contains 11 NA; column Rear.seat.room contains 2 NA.
#### Model selection
pairs(~.,data=Cars93,panel = panel.smooth)
#### There is  strong linear relationship between Price and Horsepower.
cor(Cars93$Price,Cars93$Horsepower) 
#### It gives 0.7882176 as result.
#### Forward selection using F-test
#### Remove Model column form model selection as it can be considered a sort of ID having 93 different values.
length(Cars93$Model)
#### Result= 93
length(unique(Cars93$Model))
#### Result 93
#### We remove Make as it is a clone of Manufacturer.
Cars93<-Cars93[,-which(colnames(Cars93) %in% c("Make","Model"))]
base_model<-lm(Price~1,data=Cars93)
formula<-as.formula(paste("~",paste(colnames(Cars93[,-which(colnames(Cars93)%in% c("Price"))]),collapse="+")))
add1(base_model,scope=formula,test="F")

#### 1	HorsePower and weight have the smallest p-value. We choose Horsepower, because of the strong linear relationship between Price and HorsePower.
#### Update the new_model
new_model<-update(base_model, .~.+Horsepower)
add1(new_model,scope=formula,test="F")
#### Manufacturer has the smallest p-value associated to the extra sum of square(F)
new_model<-update(new_model, .~.+Manufacturer)
summary(new_model)
add1(new_model,scope=formula,test="F")
#### Type has the smallest p-value
new_model<-update(new_model, .~.+Type)
summary(new_model)
#### we still have significant predictor in new_model
add1(new_model,scope=formula,test="F")
#### No more significant predictor. new_model formula contains the following variables:
lm(formula = Price ~ Horsepower + Manufacturer + Type, data = Cars93)
model_F1<-new_model

#### 2	Let's use weight as first predictor and see how the model changes. 
new_model<-update(base_model, .~.+Weight)
summary(new_model)
add1(new_model,scope=formula,test="F")
#### Manufacturer has the smallest p-value associated to the extra sum of square(F)
new_model<-update(new_model, .~.+Manufacturer)
summary(new_model)
add1(new_model,scope=formula,test="F")
#### Type has the smallest p-value
new_model<-update(new_model, .~.+Type)
summary(new_model)
add1(new_model,scope=formula,test="F")
#### Horsepower has the smallest p-value
new_model<-update(new_model, .~.+Horsepower)
summary(new_model)
#### Weight is not longer significant, so we remove weight from the model.
new_model<-update(new_model, .~.-Weight)
add1(new_model,scope=formula,test="F")
#### No more significant variables can be added.
new_model formula contains the following variables:
lm(formula = Price ~ Manufacturer + Type + Horsepower, data = Cars93)
model_F2<-new_model
#### 3	Let us use a different approach, using AIC value.
#### Let us do forward/backward stepwise regression, using the AICp criterion at each step instead of F-test.
formula_full<-as.formula(paste("Price~",paste(colnames(Cars93[,-which(colnames(Cars93)%in% c("Price","Make","Model"))]),collapse="+")))
Base <- lm( Price ~ Horsepower, data=Cars93 )
Full<-lm(formula_full,data=Cars93)
Step<-step(Base, scope = list( upper=Full, lower=~1 ), direction = "both", trace=FALSE)
model_step<-lm(formula = Price ~ Horsepower + Manufacturer + Type + Width + Fuel.tank.capacity + AirBags + Cylinders, data = Cars93)
#### 4	Last approach we use Mallow index. It can be used only with numerical column. Does not work out well with categorical value.
install.packages("leaps")
library(leaps)
class<-sapply(Cars93,class)
numeric<-ifelse(class %in% c("numeric","integer"),names(class),NA)
numeric<-numeric[!is.na(numeric)]
model_Cp<-leaps(y=Cars93[,"Price"],x=Cars93[,numeric[!numeric=="Price"]])
It gives an error because of the presence of NA values into columns. Therefore we replace NA vaues with the column average.
meandf<-data.frame(lapply(Cars93[,c("Luggage.room","Rear.seat.room")],function(x){ifelse(is.na(x),round(mean(x[!is.na(x)]),0),x)}))
Cars93[,c("Luggage.room","Rear.seat.room")]<-meandf
model_Cp<-leaps(y=Cars93[,"Price"],x=Cars93[,numeric[!numeric=="Price"]])
Cpplot(model_Cp)
#### The best model seems to be 451011 is the model with the less number of variables and with MallowCp~p(number of predictors of the model) . The mode uses variables
#### 4:Horsepower
#### 5:RPM
#### 10:Wheelbase
#### 11:Width
model_Mallow<-lm(Price~Horsepower+RPM+Wheelbase+Width,data=Cars93)
#### We could aslo using backward elination model, but for now we have enough material to work on :)
#### Finally we have: model_F1, model_F2,model_step,model_Mallow
In linear regression most of the information regarding the model are included into the residuals plot.
Assumptions for linear regression: The error terms follow a normal probability distribution centered at zero with a fixed variance. 
Even though the calculations of the regression model and R2 do not depend on the normality assumption, identifying patterns in residual plots can often lead to another model that better explains the response variable.
par(mfrow=c(2, 2))
plot(model_F1$fit,model_F1$res,xlab="Fitted F1",ylab="Residuals F1")
plot(model_F2$fit,model_F2$res,xlab="Fitted F2",ylab="Residuals F2")
plot(model_step$fit,model_step$res,xlab="Fitted step",ylab="Residuals step")
plot(model_Mallow$fit,model_Mallow$res,xlab="Fitted Mallow",ylab="Residuals Mallow")
library(car)
par(mfrow=c(2, 2))
qqPlot(model_F2$fit)
qqPlot(model_F1$fit)
qqPlot(model_step$fit)
qqPlot(model_Mallow$fit)
par(mfrow=c(2, 2))
plot(model_F1$fit,model_F1$res,xlab="Fitted F1",ylab="Residuals F1")
plot(model_F2$fit,model_F2$res,xlab="Fitted F2",ylab="Residuals F2")
plot(model_step$fit,model_step$res,xlab="Fitted step",ylab="Residuals step")
plot(model_Mallow$fit,model_Mallow$res,xlab="Fitted Mallow",ylab="Residuals Mallow")
Plot of the residuals look a bit funnelled. We can correct it using log(Price) insted of Price.
model_F1log<-lm(formula = log(Price,10) ~ Horsepower + Manufacturer + Type, data = Cars93)
model_F2log<-lm(formula = log(Price,10) ~ Manufacturer + Type + Horsepower, data = Cars93)
model_steplog<-lm(formula = log(Price,10) ~ Horsepower + Manufacturer + Type + Width + Fuel.tank.capacity + AirBags + Cylinders, data = Cars93)
model_Mallowlog<-lm(formula = log(Price,10) ~ Horsepower + RPM + Wheelbase + Width, data = Cars93)
par(mfrow=c(2, 2))
plot(model_F1log$fit,model_F1log$res,xlab="Fitted F1",ylab="Residuals F1")
plot(model_F2log$fit,model_F2log$res,xlab="Fitted F2",ylab="Residuals F2")
plot(model_steplog$fit,model_steplog$res,xlab="Fitted step",ylab="Residuals step")
plot(model_Mallowlog$fit,model_Mallowlog$res,xlab="Fitted Mallow",ylab="Residuals Mallow")
For the Mallow3
-----
model_F1sqrt<-lm(formula = sqrt(Price) ~ Horsepower + Manufacturer + Type, data = Cars93)
model_F2sqrt<-lm(formula = sqrt(Price) ~ Manufacturer + Type + Horsepower, data = Cars93)
model_stepsqrt<-lm(formula = sqrt(Price) ~ Horsepower + Manufacturer + Type + Width + Fuel.tank.capacity + AirBags + Cylinders, data = Cars93)
model_Mallowsqrt<-lm(formula = sqrt(Price) ~ Horsepower + RPM + Wheelbase + Width, data = Cars93)
model_F1sqrt<-lm(formula = sqrt(Price) ~ Horsepower + RPM + Wheelbase + Width, data = Cars93)
plot(model_F1sqrt$fit,model_F1sqrt$res,xlab="Fitted Mallow",ylab="Residuals Mallow")
plot(density(model_F1sqrt$res))
plot(density(model_F2sqrt$res))
plot(density(model_stepsqrt$res))
plot(density(model_Mallowsqrt$res))
Looking at the plot of the residuals we prefer to use for all the models sqrt transformation of the dependent variable.
--------------
Outliers and Influential Observations
cutoff <- 4/((nrow(Cars93)-length(model_F1sqrt$coefficients)-2))
plot(model_F1sqrt, which=4, cook.levels=cutoff)
cutoff <- 4/((nrow(Cars93)-length(model_F2sqrt$coefficients)-2))
plot(model_F2sqrt, which=4, cook.levels=cutoff)
cutoff <- 4/((nrow(Cars93)-length(model_stepsqrt$coefficients)-2))
plot(model_stepsqrt, which=4, cook.levels=cutoff)
cutoff <- 4/((nrow(Cars93)-length(model_Mallowsqrt$coefficients)-2))
plot(model_Mallowsqrt, which=4, cook.levels=cutoff)
cook <- cooks.distance(model_F1sqrt)
There are not significant influential point as  the max value for cook_distance is belowe 1, and anyway we decide not 
It is possible to generate new models subsetting the training data to those points having cook distance belowe the cutoff. Anyway we have decided to not make any change as we don’t have any information regarding those influential points(registration error etc…).
Multicollinearity exists when two or more explanatory variables in a multiple regression model are highly correlated with each other.
Values of VIF >5 means possible multicollinearity. Values of VIF>10 means strong multicollinearity is certainly a problem-

vif(model_F1sqrt)
vif(model_F2sqrt)
vif(model_stepsqrt)
vif(model_Mallowsqrt)

Looking at the values only Width in the Mallowsqrt model has VIF>5. Let's check what is the variable correlated with Width.
sapply(Cars93[,c("Horsepower","RPM","Wheelbase")],function(x) {cor(x,Cars93$Width)})
Horsepower        RPM  Wheelbase 
 0.6444134 -0.5397211  0.8072134
There is quite a stron linear relationship between Width and Wheelbase and this is quite normal since the wheelbase is the distance between its front and rear wheels. It increases with the increase of the width.
More difficult is to evaluate the value of VIF for categorical value.
If our goal is to create a model to describe or predict, then multicollinearity really is not a problem and we can keep both Width and Wheelbase.
However, if our goal is to understand how a specific explanatory variable influences the response, then multicollinearity can cause coefficients (and their corresponding p-values when testing their significance) to be unreliable.
model_Mallowsqrt=lm(formula = sqrt(Price) ~ Horsepower + RPM + Wheelbase + Width,data = Cars93)
Let's see what happens when we remove Wheelbase from the model.
lm(formula = sqrt(Price) ~ Horsepower + RPM + Width,data = Cars93)
Width is no longer significant but the model was not chosen because of the significant of the predictors. Beta values almost don't change. 
R_Squared is reduced from 0.74 to 0.66. Therefore we decide to keep the variable Wheelbase.
GOODNESS OF FIT R2,F-statistic
summary(model_Mallowsqrt)=Adjusted R-squared:  0.7411,Residual standard error: 0.5154,F-statistic: 22.44 on 46 and 46 DF,  p-value: < 2.2e-16
summary(model_stepsqrt)=Adjusted R-squared:  0.9147,Residual standard error: 0.2959,F-statistic: 25.38 on 37 and 55 DF,  p-value: < 2.2e-16
summary(model_F2sqrt)=Adjusted R-squared:  0.9075,Residual standard error: 0.3081,F-statistic: 25.38 on 37 and 55 DF,  p-value: < 2.2e-16
summary(model_F1sqrt)=Adjusted R-squared:  0.9075,Residual standard error: 0.3081,F-statistic: 66.84 on 4 and 88 DF,  p-value: < 2.2e-16

Only Mallow model has lowest R-squared and highest residua standard errors and this was expected because the Mallow model considered only numeric variable. The other 3 models have very high R-squared
In all mmodels F-statistic for the whole model is highly significant.
ERROR
set.seed(12345)
rgroup<-runif(dim(Cars93)[[1]])
test<-Cars93[rgroup<0.2,]
pred_Mallowsqrt<-predict(model_Mallowsqrt,newdata = test)
pred_stepsqrt<-predict(model_stepsqrt,newdata = test)
pred_F2sqrt<-predict(model_F2sqrt,newdata = test)
pred_F1sqrt<-predict(model_F1sqrt,newdata = test)
install.packages("hydroGOF")
library(hydroGOF)
rmse(pred_F1sqrt,sqrt(test$Price))
rmse(pred_F2sqrt,sqrt(test$Price))
rmse(pred_stepsqrt,sqrt(test$Price))
rmse(pred_Mallowsqrt,sqrt(test$Price))

rmse(pred_F1sqrt,sqrt(test$Price)) = 0.2649157
rmse(pred_F2sqrt,sqrt(test$Price)) = 0.2649157
rmse(pred_stepsqrt,sqrt(test$Price)) = 0.2407184
rmse(pred_Mallowsqrt,sqrt(test$Price)) = 0.4445927

Again in prediction definetely Mallow model perform less because we have chosen only numerical variables.
Finally removing the collinearity from the model_Mallow removing the variable Wheelbase causes an higher error and therefore a worse performance of the model in prediction. Therefore for prediction it is better to keep the collinear variable
