## Filtering mobile spam with the naive Bayes algorithm
#### Data available at :"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/"
fileName <- "SMSSpamCollection"
conn <- file(fileName,open="r")
linn <- readLines(conn)
for(i in 1:length(linn)) {
	class[i]<-ifelse(substr(linn[i],1,1) == "h","ham","spam")
	text[i]<-ifelse(substr(linn[i],1,1) == "h",substr(linn[i],5,length(linn)),substr(linn[i],6,length(linn)))
}
close(conn)
df <- data.frame(class)
df$text <- unlist(text)
table(df$class)
library(tm)
#### let's create a corpus the the text variable
corpus <- Corpus(VectorSource(df$text))
inspect(corpus[1:3])
#### Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
#### Remove numbers
corpus <- tm_map(corpus,removeNumbers)
#### Convert lowercase
corpus <- tm_map(corpus,tolower)
#### Convert remove stopwords
corpus <- tm_map(corpus,removeWords, stopwords("english"))
#### remove punctuation
corpus <- tm_map(corpus, removePunctuation)
inspect(corpus[1:3])
#### tokenization
corpus_dtm <- DocumentTermMatrix(corpus)
inspect(corpus_dtm[1:3])
#### Consider only tems appearing at least 5 times in the corpus
termfrequency <- as.vector(findFreqTerms(corpus_dtm, lowfreq=10))
corpus_final <- corpus_dtm[,termfrequency]
inspect(corpus_final[1:3,])
#### Convert everything in a dataframe to be fed into naive Bayes algorithm
#### df <- data.frame(inspect(corpus_final),stringsAsFactors=F)
#### Let's create Train and Test data 
count_yesno <- function(x){ifelse(x>0,1,0); x <- factor(x, levels=c(0,1),labels=c("no","yes"))}
final_sms <- apply(corpus_final,2,count_yesno)
index <- sample(1:dim(final_sms)[1],size=0.2*dim(final_sms)[1])
test_sms <- final_sms[index,]
train_sms <- final_sms[-index,]
library(e1071)
model <- naiveBayes(train_sms,df$class[-index])
predict <- predict(model,test_sms)
table(predict,df$class[index])
library(caret)
confusionMatrix(predict,df$class[index])
#### Very good result: Accuracy : 0.9766607,Sensitivity : 0.9948718,Specificity : 0.8489209
#### The number of ham messages that were wrongly predicted as spam is 5 (false positive). 
#### The number of spam messages that were wrongly predicted as ham is 21 (false negative).
#### False positive and then sensitivity is much more important because it is more dangerous classifying an important ham mail as spam.
#### False negative and then specificity is less important because that would cause reading few spam emails. Not a big effort :)

#### Improvement using Laplace corrector
modelLaplace <- naiveBayes(train_sms,df$class[-index],laplace=1)
predictLaplace <- predict(modelLaplace,test_sms)
table(predictLaplace,df$class[index])
library(caret)
confusionMatrix(predictLaplace,df$class[index])
#### Excellent result: Accuracy : 0.9802513,Sensitivity : 0.9979487,Specificity : 0.8561151
#### The number of ham messages that were wrongly predicted as spam is reduced from 5 to 2. 
#### The number of spam messages that were wrongly predicted as ham is reduced from 21 to 20.

